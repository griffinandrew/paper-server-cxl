// Hybrid allocator using DRAM (jemalloc) up to a limit, then PMEM (UMF)
use core::alloc::{GlobalAlloc, Layout};
use std::sync::{Once, atomic::{AtomicUsize, Ordering}};
use std::ptr;
use tikv_jemallocator::Jemalloc;

mod allocator_bindings {
    include!("umf_allocator_bindings.rs"); // generated by bindgen
}

/// Hybrid allocator: first DRAM up to a limit, then PMEM
pub struct HybridGlobal;

static INIT: Once = Once::new();
static DRAM_ALLOCATED: AtomicUsize = AtomicUsize::new(0);
static mut DRAM_LIMIT: usize = 1024 * 1024 * 1024 * 1; // default 1 GiB

static ALL_MEM_ALLOCATED: AtomicUsize = AtomicUsize::new(0);

/// Per-allocation header
#[repr(C)]
struct Header {
    tag: u8,       // 0 = DRAM, 1 = PMEM

}

unsafe impl GlobalAlloc for HybridGlobal {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
    
        // Compute combined layout with header
        let header_layout = Layout::new::<Header>();
        let (combined_layout, offset) = header_layout.extend(layout).unwrap();
        //need to apply the padding apparently..... 
        let padded_combined_layout = combined_layout.pad_to_align();

        // Decide backend
        let (raw, tag) = if Self::should_use_dram(padded_combined_layout.size()) {
            let ptr = Jemalloc.alloc(padded_combined_layout);
            if ptr.is_null() { return ptr::null_mut(); }
            DRAM_ALLOCATED.fetch_add(padded_combined_layout.size(), Ordering::SeqCst);
            ALL_MEM_ALLOCATED.fetch_add(padded_combined_layout.size(), Ordering::SeqCst);
            (ptr, 0)
        } else {
            unsafe {
                INIT.call_once(|| {
                    allocator_bindings::umf_allocator_init(
                        b"/dev/dax0.0\0".as_ptr() as *const i8,
                        266_352_984_064, // approximate PMEM size
                    );
                });
        }
            let ptr = allocator_bindings::umf_alloc(padded_combined_layout.size()) as *mut u8;
            if ptr.is_null() { return ptr::null_mut(); }
            ALL_MEM_ALLOCATED.fetch_add(padded_combined_layout.size(), Ordering::SeqCst);
            (ptr, 1)
        };

        // Write header at start of allocation
        let hdr_ptr = raw as *mut Header;
        unsafe {
            ptr::write(hdr_ptr, Header {
                tag,
            });
        }

        // Return user pointer at correct alignment
        unsafe { raw.add(offset) } // i think this is still okay....
    }

    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        //if ptr.is_null() { return; }


        //this is probably wrong way of doing ittttt
        // Reconstruct the header pointer using the original layout and offset
        let header_layout = Layout::new::<Header>();
        let (combined_layout, offset) = header_layout.extend(layout).unwrap();
        let padded_combined_layout = combined_layout.pad_to_align();

        // The header is located at (ptr - offset)
        let hdr_ptr = ptr.sub(offset) as *mut Header;
        let hdr = &*hdr_ptr;

        if hdr.tag == 0 {
            // DRAM
            unsafe { Jemalloc.dealloc(hdr_ptr as *mut u8, padded_combined_layout); }
            DRAM_ALLOCATED.fetch_sub(padded_combined_layout.size(), Ordering::SeqCst);
            ALL_MEM_ALLOCATED.fetch_sub(padded_combined_layout.size(), Ordering::SeqCst);
        } else {
            //pmem
            unsafe { allocator_bindings::umf_dealloc(hdr_ptr as *mut std::ffi::c_void); }
            ALL_MEM_ALLOCATED.fetch_sub(padded_combined_layout.size(), Ordering::SeqCst);
        }
    }
}

impl HybridGlobal {
    /// Set the DRAM limit in bytes
    pub fn set_dram_limit(limit: usize) {
        unsafe { DRAM_LIMIT = limit; }
    }

    /// Determine whether to allocate from DRAM
    fn should_use_dram(size: usize) -> bool {
        let current = DRAM_ALLOCATED.load(Ordering::Relaxed);
        unsafe { current + size <= DRAM_LIMIT }
    }
}
